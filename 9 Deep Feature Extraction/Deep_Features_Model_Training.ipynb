{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ELePreM3QBkb"},"source":["### Importing required packages\n","\n","* First, we import pytorch, the deep learning library which we’ll be using, and torchvision, which provides our dataset and data transformations. \n","\n","\n","* We also import torch.nn (pytorch’s neural network library), torch.nn.functional (includes non-linear functions like ReLu and sigmoid) and torch.optim for implementing various optimization algorithms.\n","\n"]},{"cell_type":"code","metadata":{"id":"FhtLVsbdA_xx"},"source":["# import libraries\n","import torch\n","import numpy as np\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim import lr_scheduler\n","from torchvision import datasets, models, transforms\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import copy\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vbm-8HxIqvxR"},"source":["### Initializing CUDA\n","\n","CUDA is used as an interface between our code and the GPU.\n","\n","Normally, we run the code in the CPU. To run it in the GPU, we need CUDA. Check if CUDA is available:"]},{"cell_type":"code","metadata":{"id":"YHj_ZREiqvxU"},"source":["# To test whether GPU instance is present in the system of not.\n","use_cuda = torch.cuda.is_available()\n","print('Using PyTorch version:', torch.__version__, 'CUDA:', use_cuda)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DuGyWSz8q4NQ"},"source":["If it's False, then we run the program on CPU. If it's True, then we run the program on GPU.\n","\n","Let us initialize some GPU-related variables:"]},{"cell_type":"code","metadata":{"id":"m_WeWksDqvxb"},"source":["device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","device"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nG_Q01fTqvxl"},"source":["### Load MNIST data\n","\n","Now, we'll load the MNIST data. For the first time, we may have to download the data, which can take a while.\n","\n","Now, \n","\n","* We will load both the training set and the testing sets \n","\n","* We will use  transform.compose() to convert the datasets into tensors using transforms.ToTensor(). We also normalize them by setting the mean and standard deviation using transforms.Normalize().\n"]},{"cell_type":"code","source":["# Normalize with mean and std (0.1307 and 0.3081 are the mean and std of MNIST data)\n","# convert data to torch.FloatTensor\n","transform = transforms.Compose([transforms.ToTensor()])"],"metadata":{"id":"_60w4g0KTJ7R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Choose the training and test datasets\n","train_data = datasets.MNIST(root='data', train=True,\n","                                   download=True, transform=transform)\n","test_data = datasets.MNIST(root='data', train=False,\n","                                  download=True, transform=transform)"],"metadata":{"id":"bSCNCB3HTo_6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verifying mean and std of MNIST data\n","train_data.data.float().mean() / 255, train_data.data.float().std() / 255"],"metadata":{"id":"Yt-aunT4Tl3D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Number of training samples\n","len(train_data)"],"metadata":{"id":"XnXft6vnT1o3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Size of one training image\n","train_data[0][0].size()"],"metadata":{"id":"o7001YowT6EC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1AsNZKmR7VwV"},"source":["\n","\n","**torch.utils.data.DataLoader** class represents a Python iterable over a dataset, with following features.\n","\n","1. Batching the data\n","2. Shuffling the data\n","3. Load the data in parallel using multiprocessing workers.\n","\n","\n","The batches of train and test data are provided via data loaders that provide iterators over the datasets to train our models."]},{"cell_type":"code","source":["# Initializing batch size\n","batch_size = 32\n","\n","# Loading the train dataset\n","train_loader = torch.utils.data.DataLoader(dataset=train_data, \n","                                           batch_size=batch_size, \n","                                           shuffle=True)\n","\n","# Loading the test dataset\n","test_loader = torch.utils.data.DataLoader(dataset=test_data, \n","                                          batch_size=batch_size, \n","                                          shuffle=True)"],"metadata":{"id":"S6wg2dciUHRX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FWYm5tj4qvxs"},"source":["The train and test data are provided via data loaders that provide iterators over the datasets.\n","\n","The first element of training data (X_train) is a 4th-order tensor of size (batch_size, 1, 28, 28), i.e. it consists of a batch of images of size 1x28x28 pixels where '1' represents one input image channel i.e. grey scale. y_train is a vector containing the correct classes (\"0\", \"1\", ..., \"9\") for each training digit."]},{"cell_type":"code","source":["for (X_train, y_train) in train_loader:\n","    print('X_train:', X_train.size(), 'type:', X_train.type())\n","    print('y_train:', y_train.size(), 'type:', y_train.type())\n","    break"],"metadata":{"id":"Zn0ncZyhUo9z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H-gRTIl1J6MC"},"source":["### Visualize a Batch of Training Data"]},{"cell_type":"code","source":["pltsize=2\n","plt.figure(figsize=(15*pltsize, pltsize))\n","\n","for i in range(10):\n","    plt.subplot(1,10,i+1)\n","    plt.axis('off')\n","   \n","    plt.imshow(X_train[i,:,:,:].numpy().reshape(28,28), cmap=\"gray\")\n","    plt.title('Class: '+str(y_train[i]))"],"metadata":{"id":"FjLEWoCgd2fq"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ba1ER7xWJ8cz"},"source":["# obtain one batch of training images\n","dataiter = iter(train_loader)\n","images, labels = next(dataiter)\n","images = images.numpy()\n","\n","pltsize=2\n","plt.figure(figsize=(15*pltsize, pltsize))\n","\n","for i in range(10):\n","    plt.subplot(1,10,i+1)\n","    plt.axis('off')  \n","    plt.imshow(images[i,:,:,:].reshape(28,28), cmap=\"gray\")\n","    # print out the correct label for each image\n","    plt.title('Class: '+str(labels[i]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HabZuzYxJ_nH"},"source":["View an Image in More Detail"]},{"cell_type":"code","metadata":{"id":"NUtr3FpPKAVT"},"source":["img = np.squeeze(images[1])\n","\n","fig = plt.figure(figsize = (12,12)) \n","ax = fig.add_subplot(111)\n","ax.imshow(img, cmap='gray')\n","width, height = img.shape\n","thresh = img.max()/2.5\n","for x in range(width):\n","    for y in range(height):\n","        val = round(img[x][y],2) if img[x][y] !=0 else 0\n","        ax.annotate(str(val), xy=(y,x),\n","                    horizontalalignment='center',\n","                    verticalalignment='center',\n","                    color='white' if img[x][y]<thresh else 'black')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u664CO5Zqvx7"},"source":["### Define the Neural Network Architecture and Optimizer\n","\n","Let's define the network as a Python class.\n","\n","There are three functions that are defined in this class:\n","\n","- ### **\\__init__()**:\n","In this function, we shall declare all the layers of our neural network, including the number of neurons, non-linear activations, etc.\n","\n","- ### **forward()**:\n","This is the function that is used to compute forward pass of the network. Here, we shall connect the different layers we had defined in \\__init__(), according to the network architecture we want to make. In this case, $x -> fc1 -> relu -> fc2 -> out$.\n","\n","\"forward\" can be called by calling the object of this class directly. For example:\n","\n","```\n","model = Net()\n","out = model(x)\n","```\n","\n","\n","- ### **backward()**:\n","This function is used to compute gradients across the entire network, and is called from the loss function at the end of the network.\n","\n","```\n","loss.backward()\n","```\n","\n","We have to write the **\\__init__()** and **forward()** methods, and PyTorch will automatically generate a **backward()** method for computing the gradients for the backward pass.\n","\n","In this case, we pass input (X) through the first layer, pass it’s output through the Relu layer, pass it's output through second layer, pass it's output to the relu layer, pass it's output through the third layer, pass it's output through the log softmax layer."]},{"cell_type":"code","metadata":{"id":"ATwFD5sdi9-c"},"source":["class Net_pretrained(nn.Module):\n","    def __init__(self):\n","        super(Net_pretrained, self).__init__()\n","        # linear layer (784 -> 1 hidden node)\n","        self.fc1 = nn.Linear(28 * 28, 512) # First fully connected layer which takes input image 28x28 --> 784\n","        self.fc2 = nn.Linear(512, 512)\n","        self.fc3 = nn.Linear(512, 512)\n","        self.fc4 = nn.Linear(512, 512)\n","        self.fc5 = nn.Linear(512, 10) # Last fully connected layer which outputs our 10 labels\n","\n","    def forward(self, x):\n","        # The view function is meant to flatten the tensor (28x28 is converted to 784)  \n","        x = x.view(-1, 28 * 28)\n","        # Add hidden layer, with relu activation function\n","        # Relu an activation function which allows positive values to pass through the network, whereas negative values are modified to zero\n","        x1 = F.relu(self.fc1(x))\n","        x2 = F.relu(self.fc2(x1))\n","        x3 = F.relu(self.fc3(x2))\n","        x4 = F.relu(self.fc4(x3))\n","        output = self.fc5(x4)\n","        return output, x4"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sSF28DZSFITy"},"source":["Let us declare an object of class Net, and make it a CUDA model if CUDA is available:"]},{"cell_type":"markdown","metadata":{"id":"nw3kD1Kjqvx_"},"source":["### Calling the instances of the network\n","\n","Let us declare an object of class Net, and make it a CUDA model if CUDA is available:"]},{"cell_type":"code","source":["model_pretrained = Net_pretrained()\n","model_pretrained = model_pretrained.to(device) "],"metadata":{"id":"49oBN7nZWUtX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(model_pretrained)"],"metadata":{"id":"JT1FtMKbWi_4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To get the parameter count of each layer, PyTorch has model.named_paramters() that returns an iterator of both the parameter name and the parameter itself."],"metadata":{"id":"0gsF5LpxXVq8"}},{"cell_type":"code","source":["from prettytable import PrettyTable\n","\n","def count_parameters(model):\n","    table = PrettyTable([\"Modules\", \"Parameters\"])\n","    total_params = 0\n","    for name, parameter in model_pretrained.named_parameters():\n","        if not parameter.requires_grad: continue\n","        # calculate only the trainable parameters:\n","        params = parameter.numel()\n","        table.add_row([name, params])\n","        # sum the number of elements for every parameter group:\n","        total_params+=params\n","    print(table)\n","    print(f\"Total Trainable Params: {total_params}\")\n","    return total_params\n","    \n","count_parameters(model_pretrained)"],"metadata":{"id":"nbYgJzL6XDr7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tEOmTzYgqvyO"},"source":["### Declaring loss function the optimizer\n","\n","In loss = nn.CrossEntropyLoss(), we pass output and the target as parameters where, output is the model prediction (what the model predicted on giving an image/data) and target is the actual label of the given image. We can see that PyTorch’s cross entropy function applies a softmax funtion to the output layer and then calculates the log loss.\n","\n","This criterion computes the cross entropy loss between input and target.\n","\n","Finally, we define an optimizer to update the model parameters based on the computed gradients. We select stochastic gradient descent (with momentum) as the optimization algorithm, and set the learning rate to 0.01. \n","\n","**Note** that there are several different options for the optimizer in PyTorch that we could use instead of SGD."]},{"cell_type":"code","metadata":{"id":"gobdMCw_vIPg"},"source":["learning_rate = 0.01\n","\n","# specify loss function\n","criterion = nn.CrossEntropyLoss()\n","\n","# specify optimizer\n","optimizer = torch.optim.SGD(model_pretrained.parameters(), lr=learning_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F75Z3ABdqvyY"},"source":["### Training and Testing the model\n","\n","Let's now define functions to train() and test() the model.\n","\n","In Training Phase, we iterate over a batch of images in the train_loader. For each batch, we perform  the following steps:\n","\n","* First we zero out the gradients using zero_grad()\n","\n","* We pass the data to the model i.e. we perform forward pass by calling the forward()\n","\n","* We calculate the loss using the actual and predicted labels\n","\n","* Perform Backward pass using backward() to update the weights"]},{"cell_type":"code","source":["def train(epoch, log_interval=100):\n","    # First switch the module mode to model.train() so that new weights can be learned after every epoch. \n","    model_pretrained.train()\n","\n","    # Loop through each batch of images in train set\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","       \n","        data, target = data.to(device), target.to(device)\n","\n","        # Zero out the gradients from the preivous step \n","        optimizer.zero_grad()\n","\n","        # Forward pass (this calls the \"forward\" function within Net)\n","        output, _ = model_pretrained(data)\n","\n","        # Compute the Loss\n","        loss = criterion(output, target)\n","\n","        # Do backward pass\n","        loss.backward()\n","\n","        # optimizer.step() updates the weights accordingly\n","        optimizer.step()\n","\n","        if batch_idx % log_interval == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.item()))"],"metadata":{"id":"cZEaASXaFCbf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R8R0CL5Gqvye"},"source":["Now we are ready to train our model using the train() function. An epoch means one pass through the whole training data. After each epoch, we evaluate the model using test():\n","\n"]},{"cell_type":"markdown","metadata":{"id":"joWkArDlBXu9"},"source":["In Testing Phase, we iterate over a batch of images in the test_loader. For each batch we perform the following steps:\n","\n","* We pass the images through the model (network) to get the outputs\n","* Pick the class / label with the highest probability\n","* Calculate the accuracy"]},{"cell_type":"code","source":["def test(loss_vector, accuracy_vector):\n","    model_pretrained.eval()                           # model.eval() here sets the PyTorch module to evaluation mode. \n","                                           \n","    test_loss, correct = 0, 0\n","\n","    for data, target in test_loader:\n","        data, target = data.to(device), target.to(device)  # Convert the data and target to Pytorch tensor \n","\n","        # Passing images/data to the model, which return the probabilites as outputs\n","        output,_ = model_pretrained(data) \n","\n","        # calculate the loss\n","        test_loss += criterion(output, target).item()\n","\n","        # convert output with maximum probabilities to predicted class\n","        # # get the index of the max log-probability\n","        _, pred = torch.max(output, 1)\n","\n","        # compare predictions to true label\n","        correct += (pred == target).sum().item()\n","    \n","    # Calculating the loss\n","    test_loss /= len(test_loader)\n","    loss_vector.append(test_loss)\n","\n","    # Calculating the accuracy\n","    accuracy = 100. * correct / len(test_loader.dataset)\n","\n","    accuracy_vector.append(accuracy)\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset), accuracy))\n","    return accuracy_vector"],"metadata":{"id":"UFgunA2EePfJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","epochs = 10\n","\n","lossv, accv = [], []\n","acc_vector = []\n","for epoch in range(1, epochs + 1):\n","    train(epoch)\n","    acc_vector = test(lossv, accv)"],"metadata":{"id":"PSL92unnnMhi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X6iurGRoqvyr"},"source":["Thus, GPU is much faster!\n","\n","Let's now visualize how the training progressed.\n","\n","Loss is a function of the difference of the network output and the target values. We are minimizing the loss function during training so it should decrease over time."]},{"cell_type":"markdown","metadata":{"id":"3aGuO2P8Hj8D"},"source":["### Plotting epoch vs test error"]},{"cell_type":"code","metadata":{"id":"htZANhIUqvys"},"source":["plt.figure(figsize=(8,5))\n","plt.plot(np.arange(1,epochs+1), lossv)\n","plt.title('test loss')\n","plt.xlabel(\"epoch\")\n","plt.ylabel(\"error\");"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UOnCOPdRMRWN"},"source":["### Save the trained model\n","\n","**Note:** Refer to the following [link](https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html) to save the pytorch models using `state_dict()`"]},{"cell_type":"code","metadata":{"id":"2gNxeP_qL6Uq"},"source":["# Specify a path\n","PATH = \"pretrained_layer4_512n.pt\"\n","\n","# Save the pytorch trained model\n","torch.save(model_pretrained.state_dict(), PATH)\n","\n","# Load\n","# model_pretrained = Net_pretrained()\n","# model_pretrained.load_state_dict(torch.load(PATH))\n","# model_pretrained.eval()"],"execution_count":null,"outputs":[]}]}